{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zRPP0IG3wib"
      },
      "outputs": [],
      "source": [
        "#LAND USE LAND COVER SHAPEFILES\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from shapely.geometry import Point\n",
        "import os\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = \"preprocessed_data.csv\"\n",
        "df = pd.read_excel(file_path, engine='openpyxl')\n",
        "\n",
        "# Define the LULC class mapping and colors\n",
        "lulc_mapping = {0: 'Waterbodies', 1: 'Cropland', 2: 'Natural Forest', 3: 'Bareland', 4: 'Grasslands'}\n",
        "lulc_colors = {\n",
        "    'Waterbodies': 'blue',   # Water - Blue\n",
        "    'Cropland': 'yellow',    # Cropland - Yellow\n",
        "    'Natural Forest': '#006400',  # Dark Green for Forest\n",
        "    'Bareland': 'brown',     # Brown for Bareland\n",
        "    'Grasslands': '#ADFF2F'  # Light Yellow-Green for Grasslands\n",
        "}\n",
        "\n",
        "# Extract only LULC columns\n",
        "lulc_columns = ['LULC1995', 'LULC2000', 'LULC2005', 'LULC2011', 'LULC2016', 'LULC2020', 'LULC2023']\n",
        "\n",
        "# Ensure the dataset contains latitude and longitude\n",
        "required_cols = lulc_columns + ['latitude', 'longitude']\n",
        "missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
        "\n",
        "# Map numerical LULC values to descriptive class names\n",
        "for year in lulc_columns:\n",
        "    df[year] = df[year].map(lulc_mapping)\n",
        "\n",
        "# Convert to GeoDataFrame using lat/lon for spatial data\n",
        "df['geometry'] = df.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
        "gdf = gpd.GeoDataFrame(df, geometry='geometry', crs=\"EPSG:4326\")  # WGS 84 CRS\n",
        "\n",
        "# Create output directory for the final image\n",
        "output_dir = \"lulc_maps/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Define the number of rows and columns for the grid layout\n",
        "fig, axes = plt.subplots(2, 4, figsize=(24, 14))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Generate maps for each LULC year and place in subplots\n",
        "for i, year in enumerate(lulc_columns):\n",
        "    ax = axes[i]\n",
        "\n",
        "    # Shift images AND TITLES from 2016 to 2023 slightly downward\n",
        "    if year in ['LULC2016', 'LULC2020', 'LULC2023']:\n",
        "        shift_amount = 0.04\n",
        "        ax.set_position([ax.get_position().x0, ax.get_position().y0 - shift_amount,\n",
        "                         ax.get_position().width, ax.get_position().height])\n",
        "\n",
        "    # Plot each LULC class with its corresponding color\n",
        "    for lulc_class, color in lulc_colors.items():\n",
        "        subset = gdf[gdf[year] == lulc_class]\n",
        "        subset.plot(ax=ax, color=color, markersize=15, alpha=0.7)  # Increase marker size for clarity\n",
        "\n",
        "    # Customize each subplot with **Larger & Bold Labels**\n",
        "    title_position = 1.02 if year in ['LULC2016', 'LULC2020', 'LULC2023'] else 1.05  # Adjust title position\n",
        "    ax.set_title(f\"{year}\", fontsize=22, color='black', fontweight='bold', pad=20, loc='center', y=title_position)\n",
        "\n",
        "    ax.set_xlabel(\"Longitude\", fontsize=18, color='black', fontweight='bold', labelpad=10)\n",
        "    ax.set_ylabel(\"Latitude\", fontsize=18, color='black', fontweight='bold', labelpad=10)\n",
        "\n",
        "    # Customize axis tick labels (Latitude/Longitude values)\n",
        "    ax.tick_params(axis='both', which='major', labelsize=16, labelcolor='black')\n",
        "\n",
        "    # Make the tick labels (longitude and latitude values) **bold**\n",
        "    for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
        "        label.set_fontsize(16)\n",
        "        label.set_fontweight('bold')\n",
        "        label.set_color('black')\n",
        "\n",
        "# Remove extra subplots if any\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "# Create a SINGLE legend (key) outside the plots\n",
        "legend_patches = [mpatches.Patch(color=color, label=label) for label, color in lulc_colors.items()]\n",
        "fig.legend(handles=legend_patches, title=\"LULC Classes\", title_fontsize=18, fontsize=18, fontweight='bold'\n",
        "           loc=\"lower center\", ncol=5, frameon=False)\n",
        "\n",
        "# Adjust layout and save as a single image\n",
        "plt.tight_layout(rect=[0, 0.07, 1, 1])\n",
        "final_png_path = os.path.join(output_dir, \"LULC_1995_2023_combined.png\")\n",
        "plt.savefig(final_png_path, dpi=400, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"âœ… Combined LULC image with improved spacing saved at {final_png_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LAND USE LAND COVER(LULC) TRENDS OVER TIME\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('preprocessed_data.csv')\n",
        "\n",
        "# Map numerical LULC codes to class names\n",
        "lulc_classes = {\n",
        "    0: 'Waterbodies',\n",
        "    1: 'Cropland',\n",
        "    2: 'Natural Forest',\n",
        "    3: 'Bareland',\n",
        "    4: 'Grasslands'\n",
        "}\n",
        "\n",
        "# List of LULC years\n",
        "years = ['LULC1995', 'LULC2000', 'LULC2005', 'LULC2011', 'LULC2016', 'LULC2020', 'LULC2023']\n",
        "year_values = [1995, 2000, 2005, 2011, 2016, 2020, 2023]\n",
        "\n",
        "# Calculate the area (count) of each LULC class for each year\n",
        "lulc_counts = {}\n",
        "for year in years:\n",
        "    counts = df[year].value_counts().reindex(lulc_classes.keys(), fill_value=0)\n",
        "    lulc_counts[year] = counts\n",
        "\n",
        "# Convert counts to DataFrame and map class names\n",
        "lulc_df = pd.DataFrame(lulc_counts)\n",
        "lulc_df.index = lulc_df.index.map(lulc_classes)\n",
        "\n",
        "# Plot all trends on a single graph\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Trend analysis using Linear Regression\n",
        "for lulc_class in lulc_df.index:\n",
        "    y = lulc_df.loc[lulc_class].values.reshape(-1, 1)\n",
        "    X = np.array(year_values).reshape(-1, 1)\n",
        "\n",
        "    # Fit linear regression model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Plot actual values\n",
        "    plt.plot(year_values, y.flatten(), 'o-', linewidth=2)\n",
        "\n",
        "    # Plot trend line (dotted)\n",
        "    plt.plot(year_values, model.predict(X).flatten(), '--', color='gray', linewidth=1.5)\n",
        "\n",
        "# Customize plot with larger fonts\n",
        "plt.title('LULC Class Trends Over Time (1995-2023)', fontsize=20, fontweight='bold')\n",
        "plt.xlabel('Year', fontsize=18, fontweight='bold')\n",
        "plt.ylabel('Area Count', fontsize=18, fontweight='bold')\n",
        "\n",
        "# Set font size for tick labels\n",
        "plt.xticks(fontsize=16, fontweight='bold')\n",
        "plt.yticks(fontsize=16, fontweight='bold')\n",
        "\n",
        "# Add a note for trend lines\n",
        "plt.text(1995, max(lulc_df.max()) * 0.95, 'Dashed lines represent trends', fontsize=14, color='Black', fontweight='bold')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YaVaO_Uk40iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEMPORAL NDVI TRENDS ACROSS LULC CATEGORIES\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the reshaped dataset\n",
        "file_path = \"preprocessed_data.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Check if 'LULC' column exists in the DataFrame\n",
        "if 'LULC' not in df.columns:\n",
        "    # Assuming the LULC column is named differently, for example, 'LULC2023'\n",
        "    lulc_column_name = 'LULC2023'  # Replace with the correct column name\n",
        "    df.rename(columns={lulc_column_name: 'LULC'}, inplace=True)\n",
        "\n",
        "# Extract LULC category names\n",
        "lulc_classes = {0: \"Waterbodies\", 1: \"Cropland\", 2: \"Natural Forest\", 3: \"Bareland\", 4: \"Grasslands\"}\n",
        "df['LULC_Category'] = df['LULC'].map(lulc_classes)\n",
        "\n",
        "# Extract years from column names and create a 'Year' column\n",
        "years = [int(col.replace('NDVI', '')) for col in df.columns if col.startswith('NDVI')]\n",
        "ndvi_df = pd.melt(df, id_vars=['LULC_Category'], value_vars=[f'NDVI{year}' for year in years],\n",
        "                  var_name='Year', value_name='NDVI')\n",
        "ndvi_df['Year'] = ndvi_df['Year'].str.extract('(\\d+)').astype(int)\n",
        "\n",
        "# Group by year and LULC category and calculate mean NDVI\n",
        "temporal_trends = ndvi_df.groupby(['Year', 'LULC_Category'])['NDVI'].mean().reset_index()\n",
        "\n",
        "# Plot NDVI trends over time for each LULC category\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=temporal_trends, x='Year', y='NDVI', hue='LULC_Category', marker=\"o\")\n",
        "\n",
        "# Set font size and bold style for better visibility\n",
        "plt.xlabel(\"Year\", fontsize=18, fontweight='bold')\n",
        "plt.ylabel(\"Mean NDVI\", fontsize=18, fontweight='bold')\n",
        "plt.title(\"Temporal NDVI Trends Across LULC Categories\", fontsize=20, fontweight='bold')\n",
        "\n",
        "# Set font size for tick labels\n",
        "plt.xticks(fontsize=16, fontweight='bold')\n",
        "plt.yticks(fontsize=16, fontweight='bold')\n",
        "\n",
        "# Move legend to the right outside the plot with a larger font size\n",
        "plt.legend(title=\"LULC Category\", title_fontsize=18, fontsize=16, loc='upper left', bbox_to_anchor=(1, 1))\n",
        "\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uY_kOR4l4NpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#VISUALIZATION OF NDVI DISTRIBUTION ACROSS LULC CATEGORIES\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"preprocessed_data.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Reshape the dataset\n",
        "years = [1995, 2000, 2005, 2011, 2016, 2020, 2023]\n",
        "reshaped_data = []\n",
        "\n",
        "for year in years:\n",
        "    if f'NDVI{year}' in df.columns:\n",
        "        temp_df = pd.DataFrame({\n",
        "            'Year': year,\n",
        "            'NDVI': df[f'NDVI{year}'],\n",
        "            'LULC': df[f'LULC{year}'].astype(int) if f'LULC{year}' in df.columns else np.nan,\n",
        "            'TMAX': df[f'TMAX{year}'] if f'TMAX{year}' in df.columns else np.nan,\n",
        "            'TMIN': df[f'TMIN{year}'] if f'TMIN{year}' in df.columns else np.nan,\n",
        "            'Prec': df[f'Prec{year}'] if f'Prec{year}' in df.columns else np.nan\n",
        "        })\n",
        "        reshaped_data.append(temp_df)\n",
        "\n",
        "# Concatenate all years into a single dataset\n",
        "df_reshaped = pd.concat(reshaped_data, ignore_index=True)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df_reshaped.dropna(inplace=True)\n",
        "\n",
        "# Feature Engineering\n",
        "# Compute rolling averages for temperature and precipitation\n",
        "df_reshaped['TMAX_Rolling'] = df_reshaped['TMAX'].rolling(window=3, min_periods=1).mean()\n",
        "df_reshaped['TMIN_Rolling'] = df_reshaped['TMIN'].rolling(window=3, min_periods=1).mean()\n",
        "df_reshaped['Prec_Rolling'] = df_reshaped['Prec'].rolling(window=3, min_periods=1).mean()\n",
        "\n",
        "# Include lagged NDVI values\n",
        "df_reshaped['NDVI_Lag1'] = df_reshaped['NDVI'].shift(1)\n",
        "df_reshaped['NDVI_Lag2'] = df_reshaped['NDVI'].shift(2)\n",
        "df_reshaped.dropna(inplace=True)\n",
        "\n",
        "# Define features and target variable\n",
        "X = df_reshaped[['LULC', 'TMAX', 'TMIN', 'Prec', 'TMAX_Rolling', 'TMIN_Rolling', 'Prec_Rolling', 'NDVI_Lag1', 'NDVI_Lag2']]\n",
        "y = df_reshaped['NDVI']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid for hyperparameter tuning\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Perform Randomized Search for faster tuning\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist, n_iter=10, cv=3, scoring='r2', n_jobs=-1, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model from Randomized Search\n",
        "best_rf_model = random_search.best_estimator_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'Optimized RMSE: {rmse}')\n",
        "print(f'Optimized RÂ² Score: {r2}')\n",
        "\n",
        "# Feature importance\n",
        "plt.figure(figsize=(8,6))\n",
        "importances = best_rf_model.feature_importances_\n",
        "plt.barh(['LULC', 'TMAX', 'TMIN', 'Prec', 'TMAX_Rolling', 'TMIN_Rolling', 'Prec_Rolling', 'NDVI_Lag1', 'NDVI_Lag2'], importances)\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Feature Importance in Optimized Random Forest Model\")\n",
        "plt.show()\n",
        "\n",
        "# Extract LULC class-based insights\n",
        "lulc_classes = {0: \"Waterbodies\", 1: \"Cropland\", 2: \"Natural Forest\", 3: \"Bareland\", 4: \"Grasslands\"}\n",
        "df_reshaped['LULC_Category'] = df_reshaped['LULC'].map(lulc_classes)\n",
        "\n",
        "# Compute mean NDVI per LULC category\n",
        "lulc_ndvi_means = df_reshaped.groupby('LULC_Category')['NDVI'].mean().sort_values()\n",
        "\n",
        "# Visualize NDVI distribution across LULC categories\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(x='LULC_Category', y='NDVI', data=df_reshaped, order=lulc_ndvi_means.index)\n",
        "plt.xlabel(\"LULC Category\")\n",
        "plt.ylabel(\"NDVI\")\n",
        "plt.title(\"NDVI Distribution Across LULC Categories\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "print(\"Mean NDVI per LULC Category:\")\n",
        "print(lulc_ndvi_means)\n",
        "\n",
        "# Plot NDVI trends over time for each LULC category\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=df_reshaped.groupby(['Year', 'LULC_Category'])['NDVI'].mean().reset_index(),\n",
        "             x='Year', y='NDVI', hue='LULC_Category', marker=\"o\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Mean NDVI\")\n",
        "plt.title(\"Temporal NDVI Trends Across LULC Categories\")\n",
        "plt.legend(title=\"LULC Category\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Extract key insights from temporal trends\n",
        "ndvi_trends = df_reshaped.groupby(['Year', 'LULC_Category'])['NDVI'].mean().unstack()\n",
        "print(\"Temporal NDVI Trends:\")\n",
        "print(ndvi_trends)\n"
      ],
      "metadata": {
        "id": "Pg3XZMkXHx_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CONFUSION MATRIX\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"preprocessed_data.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define features and target variable (latest NDVI value)\n",
        "target = 'NDVI2023'\n",
        "\n",
        "# Define feature base names (without years)\n",
        "feature_bases = ['LULC', 'TMAX', 'TMIN', 'Prec', 'BSI', 'SMI']\n",
        "\n",
        "# Identify available years in the dataset\n",
        "years = [1995, 2000, 2005, 2011, 2016, 2020, 2023]\n",
        "\n",
        "# Compute averaged features\n",
        "for feature in feature_bases:\n",
        "    cols = [f'{feature}{year}' for year in years if f'{feature}{year}' in df.columns]\n",
        "    if cols:\n",
        "        df[feature + '_avg'] = df[cols].mean(axis=1)\n",
        "\n",
        "# Define final feature set using the averaged values\n",
        "features = [f'{feature}_avg' for feature in feature_bases]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Initialize and train the XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = rf_model.predict(X_test)  # Predictions for Random Forest\n",
        "y_pred_xgb = xgb_model.predict(X_test)  # Predictions for XGBoost\n",
        "\n",
        "# Convert predictions to integers for confusion matrix\n",
        "y_pred_rf = y_pred_rf.astype(int)\n",
        "y_pred_xgb = y_pred_xgb.astype(int)\n",
        "y_test = y_test.astype(int)\n",
        "\n",
        "# Generate confusion matrices\n",
        "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
        "\n",
        "# Plot confusion matrix for Random Forest\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={\"size\": 14, \"weight\": \"bold\"})\n",
        "plt.title(\"Confusion Matrix - Random Forest\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Predicted\", fontsize=12, fontweight='bold')\n",
        "plt.ylabel(\"Actual\", fontsize=12, fontweight='bold')\n",
        "\n",
        "# Plot confusion matrix for XGBoost\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(conf_matrix_xgb, annot=True, fmt='d', cmap='Greens', cbar=False, annot_kws={\"size\": 14, \"weight\": \"bold\"})\n",
        "plt.title(\"Confusion Matrix - XGBoost\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Predicted\", fontsize=12, fontweight='bold')\n",
        "plt.ylabel(\"Actual\", fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-DySdg664Ecx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CLASSIFICATION METRICS FOR CLASS\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Replace these arrays with your actual predictions and labels from the Random Forest model\n",
        "y_true = np.array([1, 2, 0, 3, 4, 2, 1, 0])\n",
        "y_pred = np.array([1, 2, 0, 3, 4, 2, 0, 0])\n",
        "\n",
        "# Ensure integer labels for consistency\n",
        "y_true = y_true.astype(int)\n",
        "y_pred = y_pred.astype(int)\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(\n",
        "    y_true, y_pred,\n",
        "    target_names=[\"Waterbodies\", \"Cropland\", \"Natural Forest\", \"Bareland\", \"Grasslands\"],\n",
        "    digits=2\n",
        ")\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ],
      "metadata": {
        "id": "s_pcyQ0vD1C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RANDOM FOREST SCORE USING LAND USE LAND COVER VARIABLE ONLY\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"preprocessed_data.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Reshape the dataset\n",
        "years = [1995, 2000, 2005, 2011, 2016, 2020, 2023]\n",
        "reshaped_data = []\n",
        "\n",
        "for year in years:\n",
        "    temp_df = pd.DataFrame({\n",
        "        'Year': year,\n",
        "        'NDVI': df[f'NDVI{year}'],\n",
        "        'LULC': df[f'LULC{year}'].astype(int)\n",
        "    })\n",
        "    reshaped_data.append(temp_df)\n",
        "\n",
        "# Concatenate all years into a single dataset\n",
        "df_reshaped = pd.concat(reshaped_data, ignore_index=True)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df_reshaped.dropna(inplace=True)\n",
        "\n",
        "# Define features and target variable\n",
        "X = df_reshaped[['LULC']]\n",
        "y = df_reshaped['NDVI']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'RMSE: {rmse}')\n",
        "print(f'RÂ² Score: {r2}')\n"
      ],
      "metadata": {
        "id": "Y_MPk4ylMydm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RANDOM FOREST SCORE USING LULC AND ENVIRONMENTAL VARIABLES(TEMPERATURE, PRECIPITATION, SOIL MOSITURE INDEX, BARE SOIL INDEX)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"preprocessed_data.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Reshape the dataset\n",
        "years = [1995, 2000, 2005, 2011, 2016, 2020, 2023]\n",
        "reshaped_data = []\n",
        "\n",
        "for year in years:\n",
        "    if f'NDVI{year}' in df.columns:\n",
        "        temp_df = pd.DataFrame({\n",
        "            'Year': year,\n",
        "            'NDVI': df[f'NDVI{year}'],\n",
        "            'LULC': df[f'LULC{year}'].astype(int) if f'LULC{year}' in df.columns else np.nan,\n",
        "            'TMAX': df[f'TMAX{year}'] if f'TMAX{year}' in df.columns else np.nan,\n",
        "            'TMIN': df[f'TMIN{year}'] if f'TMIN{year}' in df.columns else np.nan,\n",
        "            'Prec': df[f'Prec{year}'] if f'Prec{year}' in df.columns else np.nan\n",
        "        })\n",
        "        reshaped_data.append(temp_df)\n",
        "\n",
        "# Concatenate all years into a single dataset\n",
        "df_reshaped = pd.concat(reshaped_data, ignore_index=True)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df_reshaped.dropna(inplace=True)\n",
        "\n",
        "# Feature Engineering\n",
        "# Compute rolling averages for temperature and precipitation\n",
        "df_reshaped['TMAX_Rolling'] = df_reshaped['TMAX'].rolling(window=3, min_periods=1).mean()\n",
        "df_reshaped['TMIN_Rolling'] = df_reshaped['TMIN'].rolling(window=3, min_periods=1).mean()\n",
        "df_reshaped['Prec_Rolling'] = df_reshaped['Prec'].rolling(window=3, min_periods=1).mean()\n",
        "\n",
        "# Include lagged NDVI values\n",
        "df_reshaped['NDVI_Lag1'] = df_reshaped['NDVI'].shift(1)\n",
        "df_reshaped['NDVI_Lag2'] = df_reshaped['NDVI'].shift(2)\n",
        "df_reshaped.dropna(inplace=True)\n",
        "\n",
        "# Define features and target variable\n",
        "X = df_reshaped[['LULC', 'TMAX', 'TMIN', 'Prec', 'TMAX_Rolling', 'TMIN_Rolling', 'Prec_Rolling', 'NDVI_Lag1', 'NDVI_Lag2']]\n",
        "y = df_reshaped['NDVI']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid for hyperparameter tuning\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Perform Randomized Search for faster tuning\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist, n_iter=10, cv=3, scoring='r2', n_jobs=-1, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model from Randomized Search\n",
        "best_rf_model = random_search.best_estimator_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'Optimized RMSE: {rmse}')\n",
        "print(f'Optimized RÂ² Score: {r2}')\n"
      ],
      "metadata": {
        "id": "bTCCHggqNc4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBOOST SCORE USING (ADVANCED FEATURES)\n",
        "# Convert BSI and MSI to numeric, forcing errors to NaN (use pd.to_numeric)\n",
        "df_reshaped['BSI'] = pd.to_numeric(df_reshaped['BSI'], errors='coerce')\n",
        "df_reshaped['MSI'] = pd.to_numeric(df_reshaped['MSI'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing values\n",
        "df_reshaped.dropna(subset=['BSI', 'MSI'], inplace=True)\n",
        "\n",
        "# Define features and target variable\n",
        "X = df_reshaped[['LULC', 'TMAX', 'TMIN', 'Prec', 'TMAX_Rolling', 'TMIN_Rolling', 'Prec_Rolling', 'NDVI_Lag1', 'NDVI_Lag2', 'BSI', 'MSI']]\n",
        "y = df_reshaped['NDVI']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Manually set hyperparameters for XGBoost\n",
        "xgb_params = {\n",
        "    'n_estimators': 200,             # Number of trees in the model\n",
        "    'max_depth': 7,                  # Maximum depth of the trees\n",
        "    'learning_rate': 0.1,            # Learning rate\n",
        "    'subsample': 0.8,                # Subsample ratio of the training set\n",
        "    'colsample_bytree': 0.8,         # Subsample ratio of columns when building each tree\n",
        "    'gamma': 0.1,                    # Minimum loss reduction required to make a further partition\n",
        "    'objective': 'reg:squarederror', # Regression objective\n",
        "    'random_state': 42               # For reproducibility\n",
        "}\n",
        "\n",
        "# Train XGBoost model with manually set hyperparameters\n",
        "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'Optimized RMSE (XGBoost): {rmse}')\n",
        "print(f'Optimized RÂ² Score (XGBoost): {r2}')\n",
        "\n",
        "# Feature importance\n",
        "plt.figure(figsize=(8,6))\n",
        "importances = xgb_model.feature_importances_\n",
        "plt.barh(['LULC', 'TMAX', 'TMIN', 'Prec', 'TMAX_Rolling', 'TMIN_Rolling', 'Prec_Rolling', 'NDVI_Lag1', 'NDVI_Lag2', 'BSI', 'MSI'], importances)\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Feature Importance in Optimized XGBoost Model\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b9qUG2CJ_A0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBoost SCORE USING BASIC FEATURES\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('data_3.csv')\n",
        "\n",
        "# Checking for non-null values in each column to ensure data is loaded correctly\n",
        "print(\"\\nChecking for non-null values in the dataset:\")\n",
        "print(data.notnull().sum())\n",
        "\n",
        "# Define the features (X) and target (y)\n",
        "X = data[['NDVI1995', 'NDVI2000', 'NDVI2005', 'NDVI2011', 'NDVI2016', 'NDVI2020', 'NDVI2023',\n",
        "          'LULC1995', 'LULC2000', 'LULC2005', 'LULC2011', 'LULC2016', 'LULC2020', 'LULC2023',\n",
        "          'TMAX1990', 'TMAX1995', 'TMAX2000', 'TMAX2005', 'TMAX2011', 'TMAX2016', 'TMAX2020', 'TMAX2021',\n",
        "          'TMIN1990', 'TMIN1995', 'TMIN2000', 'TMIN2005', 'TMIN2011', 'TMIN2016', 'TMIN2020', 'TMIN2021',\n",
        "          'Prec1995', 'Prec2000', 'Prec2005', 'Prec2011', 'Prec2016', 'Prec2020', 'Prec2023',\n",
        "          'BSI1995', 'BSI2000', 'BSI2005', 'BSI2011', 'BSI2016', 'BSI2020', 'BSI2021',\n",
        "          'SMI2016', 'SMI2020', 'SMI2021']]\n",
        "\n",
        "y = data['NDVI2023']\n",
        "\n",
        "# Check the shape and first few rows of X and y\n",
        "print(f\"Shape of X: {X.shape}\")\n",
        "print(f\"Shape of y: {y.shape}\")\n",
        "\n",
        "print(\"First few rows of the dataset (X and y):\")\n",
        "print(X.head())\n",
        "print(y.head())\n",
        "\n",
        "# Ensure there are no NaN values (since we are assuming your dataset is clean)\n",
        "print(f\"Are there any NaN values in X? {X.isnull().sum().sum() > 0}\")\n",
        "print(f\"Are there any NaN values in y? {y.isnull().sum() > 0}\")\n",
        "\n",
        "# Check if X and y are not empty\n",
        "if X.shape[0] == 0 or y.shape[0] == 0:\n",
        "    raise ValueError(\"No valid data remaining. Check the dataset for valid rows.\")\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"Shape of X_train: {X_train.shape}, Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}, Shape of y_test: {y_test.shape}\")\n",
        "\n",
        "# Initialize the XGBoost model directly\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=200, max_depth=7, learning_rate=0.1,\n",
        "                             subsample=0.9, colsample_bytree=0.9, random_state=42)\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict with the XGBoost model\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"RMSE (XGBoost): {rmse_xgb}\")\n",
        "print(f\"RÂ² Score (XGBoost): {r2_xgb}\")\n"
      ],
      "metadata": {
        "id": "NdcgYubw_6DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LONG SHORT-TERM MEMORY (LSTM) SCORE\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import random\n",
        "\n",
        "# Define function to create and return the LSTM model with hyperparameters\n",
        "def create_lstm_model(units=50, dropout_rate=0.2, lstm_layers=1, batch_size=32, epochs=50):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer with the shape of the data\n",
        "    model.add(Input(shape=(look_back, X_seq.shape[2])))\n",
        "\n",
        "    # Bidirectional LSTM layer\n",
        "    model.add(Bidirectional(LSTM(units=units, return_sequences=True)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Add additional LSTM layers if needed\n",
        "    for _ in range(lstm_layers - 1):\n",
        "        model.add(LSTM(units=units, return_sequences=True))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(LSTM(units=units))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Dense layer\n",
        "    model.add(Dense(units=1))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_dist = {\n",
        "    'units': [50, 64, 128, 256],\n",
        "    'dropout_rate': [0.2, 0.3, 0.5],\n",
        "    'lstm_layers': [1, 2, 3],\n",
        "    'batch_size': [32, 64],\n",
        "    'epochs': [50, 100, 200]\n",
        "}\n",
        "\n",
        "# Random search manually\n",
        "best_rmse = float('inf')\n",
        "best_r2 = -float('inf')\n",
        "best_model = None\n",
        "\n",
        "for _ in range(10):  # Perform 10 random searches\n",
        "    # Randomly select hyperparameters\n",
        "    units = random.choice(param_dist['units'])\n",
        "    dropout_rate = random.choice(param_dist['dropout_rate'])\n",
        "    lstm_layers = random.choice(param_dist['lstm_layers'])\n",
        "    batch_size = random.choice(param_dist['batch_size'])\n",
        "    epochs = random.choice(param_dist['epochs'])\n",
        "\n",
        "    # Create and train the model\n",
        "    model = create_lstm_model(units=units, dropout_rate=dropout_rate, lstm_layers=lstm_layers, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n",
        "\n",
        "    # Predict on test data\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Store the best model\n",
        "    if rmse < best_rmse:\n",
        "        best_rmse = rmse\n",
        "        best_r2 = r2\n",
        "        best_model = model\n",
        "\n",
        "print(f\"Optimized RMSE (LSTM with Manual Hyperparameter Tuning): {best_rmse}\")\n",
        "print(f\"Optimized RÂ² Score (LSTM with Manual Hyperparameter Tuning): {best_r2}\")\n"
      ],
      "metadata": {
        "id": "mm2FRNrwApNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jjzCGJhyDDOS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}